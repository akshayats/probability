{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbWmRwcuatWz"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4NyePmVaxhL"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaMcWFBtVHG9"
   },
   "source": [
    "# TensorFlow Probability Case Study: Covariance Estimation\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Probability_Case_Study_Covariance_Estimation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Probability_Case_Study_Covariance_Estimation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rueq5va_3TYc"
   },
   "source": [
    "I wrote this notebook as a case study to learn TensorFlow Probability.  The problem I chose to solve is estimating a covariance matrix for samples of a 2-D mean 0 Gaussian random variable.  The problem has a couple of nice features:\n",
    "\n",
    "* If we use an inverse Wishart prior for the covariance (a common approach), the problem has an analytic solution, so we can check our results.\n",
    "* The problem involves sampling a constrained parameter, which adds some interesting complexity.\n",
    "* The most straightforward solution is not the fastest one, so there is some optimization work to do.\n",
    "\n",
    "I decided to write my experiences up as I went along. It took me awhile to wrap my head around the finer points of TFP, so this notebook starts fairly simply and then gradually works up to more complicated TFP features.  I ran into lots of problems along the way, and I've tried to capture both the processes that helped me identify them and the workarounds I eventually found.  I've tried to include *lots* of detail (including lots of tests to make sure individual steps are correct)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-qYKi2r0VFr"
   },
   "source": [
    "## Why learn TensorFlow Probability?\n",
    "\n",
    "I found TensorFlow Probability appealing for my project for a few reasons:\n",
    "\n",
    "* TensorFlow probability lets you prototype develop complex models interactively in a notebook.  You can break your code up into small pieces that you can test interactively and with unit tests.\n",
    "* Once you're ready to scale up, you can take advantage of all of the infrastructure we have in place for making TensorFlow run on multiple, optimized processors on multiple machines.\n",
    "* Finally, while I really like Stan, I find it quite difficult to debug.  You have to write all your modeling code in a standalone language that has very few tools for letting you poke at your code, inspect intermediate states, and so on.\n",
    "\n",
    "The downside is that TensorFlow Probability is much newer than Stan and PyMC3, so the documentation is a work in progress, and there's lots of functionality that's yet to be built.  Happily, I found TFP's foundation to be solid, and it's designed in a modular way that allows one to extend its functionality fairly straightforwardly.  In this notebook, in addition to solving the case study, I'll show some ways to go about extending TFP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZ_vkMN0Ti4Z"
   },
   "source": [
    "## Who this is for\n",
    "\n",
    "I'm assuming that readers are coming to this notebook with some important prerequisites.  You should:\n",
    "\n",
    "* Know the basics of Bayesian inference.  (If you don't, a really nice first book is *[Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/)*)\n",
    "* Have some familiarity with an MCMC sampling library, e.g. [Stan](http://mc-stan.org/) / [PyMC3](http://docs.pymc.io/) / [BUGS](https://www.mrc-bsu.cam.ac.uk/software/bugs/)\n",
    "* Have a solid grasp of [NumPy](http://www.numpy.org/)  (One good intro is *[Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do)*)\n",
    "* Have at least passing familiarity with [TensorFlow](https://www.tensorflow.org/), but not necessarily expertise.  (*[Learning TensorFlow](http://shop.oreilly.com/product/0636920063698.do)* is good, but TensorFlow's rapid evolution means that most books will be a bit dated.  Stanford's [CS20](https://web.stanford.edu/class/cs20si/) course is also good.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YButn5pt_71w"
   },
   "source": [
    "# First attempt\n",
    "\n",
    "Here's my first attempt at the problem.  Spoiler: my solution doesn't work, and it's going to take several attempts to get things right!  Although the process takes awhile, each attempt below has been useful for learning a new part of TFP.\n",
    "\n",
    "One note: TFP doesn't currently implement the inverse Wishart distribution (we'll see at the end how to roll our own inverse Wishart), so instead I'll change the problem to that of estimating a precision matrix using a Wishart prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93Jufm9K3E8R"
   },
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQ_h8ns5Inq-"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vnnN2pFUcLlF"
   },
   "source": [
    "## Step 1: get the observations together\n",
    "\n",
    "My data here are all synthetic, so this is going to seem a bit tidier than a real-world example.  However, there's no reason you can't generate some synthetic data of your own.\n",
    "\n",
    "**Tip**: Once you've decided on the form of your model, you can pick some parameter values and use your chosen model to generate some synthetic data.  As a sanity check of your implementation, you can then verify that your estimates include the true values of the parameters you chose.  To make your debugging / testing cycle faster, you might consider a simplified version of your model (e.g. use fewer dimensions or fewer samples).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ykrVMrWPN_3"
   },
   "source": [
    "**Tip:** It's easiest to work with your observations as NumPy arrays.  One important thing to note is that NumPy by default uses float64's, while TensorFlow by default uses float32's. \n",
    "\n",
    "In general, TensorFlow operations want all arguments to have the same type, and you have to do explicit data casting to change types.  If you use float64 observations, you'll need to add in a lot of cast operations.  NumPy, in contrast, will take care of casting automatically.  Hence, **it is *much* easier to convert your Numpy data into float32 than it is to force TensorFlow to use float64.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JG4jAba4TPxm"
   },
   "source": [
    "### Choose some parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4lSqTGHKAyf"
   },
   "outputs": [],
   "source": [
    "# We're assuming 2-D data with a known true mean of (0, 0)\n",
    "true_mean = np.zeros([2], dtype=np.float32)\n",
    "# We'll make the 2 coordinates correlated\n",
    "true_cor = np.array([[1.0, 0.9], [0.9, 1.0]], dtype=np.float32)\n",
    "# And we'll give the 2 coordinates different variances\n",
    "true_var = np.array([4.0, 1.0], dtype=np.float32)\n",
    "# Combine the variances and correlations into a covariance matrix\n",
    "true_cov = np.expand_dims(np.sqrt(true_var), axis=1).dot(\n",
    "    np.expand_dims(np.sqrt(true_var), axis=1).T) * true_cor\n",
    "# We'll be working with precision matrices, so we'll go ahead and compute the\n",
    "# true precision matrix here\n",
    "true_precision = np.linalg.inv(true_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ayainGqrY193",
    "outputId": "37d7b2e1-04f6-41a8-e105-f14e678bba08",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.  1.8]\n",
      " [1.8 1. ]]\n",
      "eigenvalues:  [4.843075   0.15692513]\n"
     ]
    }
   ],
   "source": [
    "# Here's our resulting covariance matrix\n",
    "print (true_cov)\n",
    "# Verify that it's positive definite, since np.random.multivariate_normal\n",
    "# complains about it not being positive definite for some reason.\n",
    "# (Note that I'll be including a lot of sanity checking code in this notebook -\n",
    "# it's a *huge* help for debugging)\n",
    "print ('eigenvalues: ', np.linalg.eigvals(true_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ef--FDjZTTlu"
   },
   "source": [
    "### Generate some synthetic observations\n",
    "\n",
    "Note that **TensorFlow Probability uses the convention that the initial dimension(s) of your data represent sample indices, and the final dimension(s) of your data represent the dimensionality of your samples.**\n",
    "\n",
    "Here we want 100 samples, each of which is a vector of length 2.  We'll generate an array `my_data` with shape (100, 2).  `my_data[i, :]` is the $i$th sample, and it is a vector of length 2.\n",
    "\n",
    "(Remember to make `my_data` have type float32!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjHoAXOlXbYi"
   },
   "outputs": [],
   "source": [
    "# Set the seed so the results are reproducible.\n",
    "np.random.seed(123)\n",
    "\n",
    "# Now generate some observations of our random variable.\n",
    "# (Note that I'm suppressing a bunch of spurious about the covariance matrix\n",
    "# not being positive semidefinite via check_valid='ignore' because it really is\n",
    "# positive definite!)\n",
    "my_data = np.random.multivariate_normal(\n",
    "    mean=true_mean, cov=true_cov, size=100,\n",
    "    check_valid='ignore').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AfkzrIrfMRVv",
    "outputId": "20ca4eb4-e683-4c20-da53-2a0146bf46cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ffvyw94VTVoP"
   },
   "source": [
    "### Sanity check the observations\n",
    "\n",
    "One potential source of bugs is messing up your synthetic data!  Let's do some simple checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "uDh3-gy_n7De",
    "outputId": "3d34157a-3913-43c2-b989-16eb7dab8dcd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do a scatter plot of the observations to make sure they look like what we\n",
    "# expect (higher variance on the x-axis, y values strongly correlated with x)\n",
    "plt.scatter(my_data[:, 0], my_data[:, 1], alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sz_9MTzgTfDZ",
    "outputId": "afd81a87-6866-4039-b90f-f5c2c185da4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of observations: [-0.2400961  -0.16638893]\n",
      "true mean: [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print ('mean of observations:', np.mean(my_data, axis=0))\n",
    "print ('true mean:', true_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "YHVcvZC-UErO",
    "outputId": "539ac047-9b51-4ddf-e3f6-0df01774d9dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance of observations:\n",
      " [[3.95307666 1.68718484]\n",
      " [1.68718484 0.94910282]]\n",
      "true covariance:\n",
      " [[4.  1.8]\n",
      " [1.8 1. ]]\n"
     ]
    }
   ],
   "source": [
    "print ('covariance of observations:\\n', np.cov(my_data, rowvar=False))\n",
    "print ('true covariance:\\n', true_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ROazOrWF6E3v"
   },
   "source": [
    "Ok, our samples look reasonble.  Next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cy2HhmCU4NZW"
   },
   "source": [
    "## Step 2: Implement the likelihood function in NumPy\n",
    "\n",
    "The main thing we'll need to write to perform our MCMC sampling in TF Probability is a log likelihood function.  In general it's a bit trickier to write TF than NumPy, so I find it helpful to do an initial implementation in NumPy.  I'm going to split the likelihood function into 2 pieces, a data likelihood function that corresponds to $P(data | parameters)$ and a prior likelihood function that corresponds to $P(parameters)$.\n",
    "\n",
    "Note that these NumPy functions don't have to be super optimized / vectorized since the goal is just to generate some values for testing.  Correctness is the key consideration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dn2KwT6y_20l"
   },
   "source": [
    "First we'll implement the data log likelihood piece.  That's pretty straightforward.  The one thing to remember is that we're going to be working with precision matrices, so we'll parameterize accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L1AvLoipOFhA",
    "outputId": "a21751c0-8b06-48f1-ccbf-c909453f7383"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-280.8182256375922"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "def log_lik_data_numpy(precision, data):\n",
    "  # np.linalg.inv is a really inefficient way to get the covariance matrix, but\n",
    "  # remember we don't care about speed here\n",
    "  cov = np.linalg.inv(precision)\n",
    "  rv = scipy.stats.multivariate_normal(true_mean, cov)\n",
    "  return np.sum(rv.logpdf(data))\n",
    "\n",
    "# test case: compute the log likelihood of the data given the true parameters\n",
    "log_lik_data_numpy(true_precision, my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KT3aTEydWseU"
   },
   "source": [
    "We're going to use a Wishart prior for the precision matrix since there's an analytical solution for the posterior (see [Wikipedia's handy table of conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions)).\n",
    "\n",
    "The [Wishart distribution](https://en.wikipedia.org/wiki/Wishart_distribution) has 2 parameters: \n",
    "\n",
    "* the number of *degrees of freedom* (labeled $\\nu$ in Wikipedia)\n",
    "* a *scale matrix* (labeled $V$ in Wikipedia)\n",
    "\n",
    "The mean for a Wishart distribution with parameters $\\nu, V$ is $E[W] = \\nu V$, and the variance is $\\text{Var}(W_{ij}) = \\nu(v_{ij}^2+v_{ii}v_{jj})$ where size of $W$ is $p \\times p$.\n",
    "\n",
    "Some useful intuition: You can generate a Wishart sample by generating $\\nu$ independent draws $x_1 \\ldots x_{\\nu}$ from a multivariate normal random variable with mean 0 and covariance $V$ and then forming the sum $W = \\sum_{i=1}^{\\nu} x_i x_i^T$.\n",
    "\n",
    "If you rescale Wishart samples by dividing them by $\\nu$, you get the sample covariance matrix of the $x_i$.  This sample covariance matrix should tend toward $V$ as $\\nu$ increases.  When $\\nu$ is small, there is lots of variation in the sample covariance matrix, so small values of $\\nu$ correspond to weaker priors and large values of $\\nu$ correspond to stronger priors.  Note that $\\nu$ must be at least as large as the dimension of the space you're sampling or you'll generate singular matrices.\n",
    "\n",
    "We'll use $\\nu = 3$ so we have a weak prior, and we'll take $V = \\frac{1}{\\nu} I$ which will pull our covariance estimate toward the identity (recall that the mean is $\\nu V$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $X=[\\mathbf x_0, \\dots, \\mathbf x_n]$ is of dimensions $p \\times \\nu$, where $p$ is the dimensionality of the modelled covariance matrix and $\\nu$ is the sample size of the multivariate normal RV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z6PB7bdt4vSL",
    "outputId": "4b70197f-dbd9-4f33-85ac-cd0f3961ca33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.103606346649766"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIOR_DF = 3\n",
    "PRIOR_SCALE = np.eye(2, dtype=np.float32) / PRIOR_DF\n",
    "\n",
    "def log_lik_prior_numpy(precision):\n",
    "  rv = scipy.stats.wishart(df=PRIOR_DF, scale=PRIOR_SCALE)\n",
    "  return rv.logpdf(precision)\n",
    "\n",
    "# test case: compute the prior for the true parameters\n",
    "log_lik_prior_numpy(true_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dj6vEWta6Nvn"
   },
   "source": [
    "The Wishart distribution is the conjugate prior for estimating the precision matrix of a multivariate normal with known mean $\\mu$.\n",
    "\n",
    "Suppose the prior Wishart parameters are $\\nu, V$ and that we have $n$ observations of our multivariate normal, $x_1, \\ldots, x_n$.  The posterior parameters are $n + \\nu, \\left(V^{-1} + \\sum_{i=1}^n (x_i-\\mu)(x_i-\\mu)^T \\right)^{-1}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibgUDLfImeZy"
   },
   "outputs": [],
   "source": [
    "n = my_data.shape[0]\n",
    "nu_prior = PRIOR_DF\n",
    "v_prior = PRIOR_SCALE\n",
    "nu_posterior = nu_prior + n\n",
    "v_posterior = np.linalg.inv(np.linalg.inv(v_prior) + my_data.T.dot(my_data))\n",
    "posterior_mean = nu_posterior * v_posterior\n",
    "v_post_diag = np.expand_dims(np.diag(v_posterior), axis=1)\n",
    "posterior_sd = np.sqrt(nu_posterior *\n",
    "                       (v_posterior ** 2.0 + v_post_diag.dot(v_post_diag.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKY0FtnjsGY-"
   },
   "source": [
    "A quick plot of the posteriors and the true values.  Note that the posteriors are close to the sample posteriors but are shrunk a bit toward the identity.  Note also that the true values are pretty far from the mode of the posterior - presumably this is because prior isn't a very good match for our data.  In a real problem we'd likely do better with something like a scaled inverse Wishart prior for the covariance (see, for example, Andrew Gelman's [commentary](http://andrewgelman.com/2012/08/22/the-scaled-inverse-wishart-prior-distribution-for-a-covariance-matrix-in-a-hierarchical-model/) on the subject), but then we wouldn't have a nice analytic posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "colab_type": "code",
    "id": "cviEd4bbeUQi",
    "outputId": "0c59a020-5f4c-4ac7-f04f-128cbab0bd94"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_precision = np.linalg.inv(np.cov(my_data, rowvar=False, bias=False))\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "fig.set_size_inches(10, 10)\n",
    "for i in range(2):\n",
    "  for j in range(2):\n",
    "    ax = axes[i, j]\n",
    "    loc = posterior_mean[i, j]\n",
    "    scale = posterior_sd[i, j]\n",
    "    xmin = loc - 3.0 * scale\n",
    "    xmax = loc + 3.0 * scale\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "    y = scipy.stats.norm.pdf(x, loc=loc, scale=scale)\n",
    "    ax.plot(x, y)\n",
    "    ax.axvline(true_precision[i, j], color='red', label='True precision')\n",
    "    ax.axvline(sample_precision[i, j], color='red', linestyle=':', label='Sample precision')\n",
    "    ax.set_title('precision[%d, %d]' % (i, j))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxOs98_0Cvqz"
   },
   "source": [
    "## Step 3: Implement the likelihood function in TensorFlow\n",
    "\n",
    "Spoiler: Our first attempt isn't going to work; we'll talk about why below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTYyufXBLOyV"
   },
   "source": [
    "**Tip**: use TensorFlow eager mode when developing your likelihood functions.  Eager mode makes TF behave more like NumPy - everything executes immediately, so you can debug interactively instead of having to use `Session.run()`.  See the notes [here](https://www.tensorflow.org/programmers_guide/eager)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iknSYSm5y73U"
   },
   "source": [
    "### Preliminary: Distribution classes\n",
    "\n",
    "TFP has a collection of distribution classes that we'll use to generate our log probabilities.  One thing to note is that these classes work with tensors of samples rather than just single samples - this allows for vectorization and related speedups.\n",
    "\n",
    "A distribution can work with a tensor of samples in 2 different ways.  It's simplest to illustrate these 2 ways with a concrete example involving a distribution with a single scalar paramter.  I'll use the [Poisson](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Poisson) distribution, which has a `rate` parameter. \n",
    "* If we create a Poisson with a single value for the `rate` parameter, a call to its `sample()` method return a single value.  This value is called an **`event`**, and in this case the events are all scalars.\n",
    "* If we create a Poisson with a tensor of values for the `rate` parameter, a call to its `sample()`method now returns multiple values, one for each value in the rate tensor.  The object acts as a *collection* of independent Poissons, each with its own rate, and each of the values returned by a call to `sample()` corresponds to one of these Poissons.  This collection of independent *but not identically distributed* events is called a **`batch`**.\n",
    "* The `sample()` method takes a `sample_shape` parameter which defaults to an empty tuple.  Passing a non-empty value for `sample_shape` results in sample returning multiple batches.  This collection of batches is called a **`sample`**.\n",
    "\n",
    "A distribution's `log_prob()` method consumes data in a manner that parallels how `sample()` generates it.  `log_prob()` returns probabilities for samples, i.e. for multiple, independent batches of events.\n",
    "  * If we have our Poisson object that was created with a scalar `rate`, each batch is a scalar, and if we pass in a tensor of samples, we'll get out a tensor of the same size of log probabilities.\n",
    "  * If we have our Poisson object that was created with a tensor of shape `T` of `rate` values, each batch is a tensor of shape `T`.  If we pass in a tensor of samples of shape D, T, we'll get out a tensor of log probabilities of shape D, T.\n",
    "\n",
    "Below are some examples that illustrate these cases.  See [this notebook](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Understanding_TensorFlow_Distributions_Shapes.ipynb) for a more detailed tutorial on events, batches, and shapes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E79nKqufBPu2"
   },
   "source": [
    "A few conventions I'll be using in this notebook:\n",
    "1. In general I'll create **new graphs for individual cells**.  That way cells are self-contained, and operations I create in one cell won't have side effects in other cells.\n",
    "2. I'll **separate graph construction from execution**.  Cells will have the form:\n",
    "  ```\n",
    "  \n",
    "  with tf.Graph().as_default() as g:\n",
    "     # construct my local graph\n",
    "     ...\n",
    "    g.finalize()  # make sure the graph doesn't change after construction\n",
    "\n",
    "  with tf.Session(graph=g) as sess:\n",
    "    # run my graph\n",
    "   ```\n",
    "3. I'll use **underscores at the end of variable names to indicate they contain the output of the TensorFlow operation without the underscore**.  For example:\n",
    "  ```\n",
    "  x = tf.add(y, z)  # x is a TensorFlow operation\n",
    "  x_ = sess.run(x)  # x_ is the output of the operation x\n",
    "  ```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2zAnT8GL534f",
    "outputId": "59903103-6dca-4460-ac81-7f040d1c9a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid draws from a single normal: [-1.4189385 -1.0439385 -0.9189385]\n",
      "draws from a batch of normals: [-1.4189385 -2.0439386 -8.918939 ]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "  # case 1: get log probabilities for a vector of iid draws from a single\n",
    "  # normal distribution\n",
    "  norm1 = tfd.Normal(loc=0., scale=1.)\n",
    "  probs1 = norm1.log_prob(tf.constant([1., 0.5, 0.]))\n",
    "\n",
    "  # case 2: get log probabilities for a vector of independent draws from\n",
    "  # multiple normal distributions with different parameters.  Note the vector\n",
    "  # values for loc and scale in the Normal constructor.\n",
    "  norm2 = tfd.Normal(loc=[0., 2., 4.], scale=[1., 1., 1.])\n",
    "  probs2 = norm2.log_prob(tf.constant([1., 0.5, 0.]))\n",
    "\n",
    "  g.finalize()\n",
    "  \n",
    "with tf.Session(graph=g) as sess:\n",
    "  print ('iid draws from a single normal:', sess.run(probs1))\n",
    "\n",
    "  print ('draws from a batch of normals:', sess.run(probs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjGMitya49UN"
   },
   "source": [
    "### Data log likelihood\n",
    "\n",
    "First we'll implement the data log likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nz1G3OviHpwg"
   },
   "source": [
    "Note: distributions can validate their input, but they don't do so by default.  We'll definitely want to turn on validation while we're debugging!  Once everything is working, we can turn validation off if speed is really critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ITlkvOvHkX5"
   },
   "outputs": [],
   "source": [
    "VALIDATE_ARGS = True\n",
    "ALLOW_NAN_STATS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2GzEpvNgatAU"
   },
   "source": [
    "One key difference from the NumPy case is that our TensorFlow likelihood function will need to handle vectors of precision matrices rather than just single matrices.  Vectors of parameters will be used when we sample from multiple chains.\n",
    "\n",
    "We'll create a distribution object that works with a batch of precision matrices (i.e. one matrix per chain).\n",
    "\n",
    "When computing log probabilities of our data, we'll need our data to be replicated in the same manner as our parameters so that there is one copy per batch variable.  The shape of our replicated data will need to be as follows:\n",
    "\n",
    "`[sample shape, batch shape, event shape]`\n",
    "\n",
    "In our case, the event shape is 2 (since we are working with 2-D Gaussians).  The sample shape is 100, since we have 100 samples.  The batch shape will just be the number of precision matrices we're working with.  It's wasteful to replicate the data each time we call the likelihood function, so we'll replicate the data in advance and pass in the replicated version.\n",
    "\n",
    "Note that this is an inefficient implementation: `MultivariateNormalFullCovariance` is expensive relative to some alternatives that we'll talk about in the optimization section at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJB5wJ1IEsBu"
   },
   "outputs": [],
   "source": [
    "def log_lik_data(precisions, replicated_data):\n",
    "  n = tf.shape(precisions)[0]  # number of precision matrices\n",
    "  # We're estimating a precision matrix; we have to invert to get log\n",
    "  # probabilities.  Cholesky inversion should be relatively efficient,\n",
    "  # but as we'll see later, it's even better if we can avoid doing the Cholesky\n",
    "  # decomposition altogether.\n",
    "  precisions_cholesky = tf.cholesky(precisions)\n",
    "  covariances = tf.cholesky_solve(precisions_cholesky,\n",
    "                                  tf.eye(2, batch_shape=[n]))\n",
    "  rv_data = tfd.MultivariateNormalFullCovariance(\n",
    "      loc=tf.zeros([n, 2]),\n",
    "      covariance_matrix=covariances,\n",
    "      validate_args=VALIDATE_ARGS,\n",
    "      allow_nan_stats=ALLOW_NAN_STATS)\n",
    "\n",
    "  return tf.reduce_sum(rv_data.log_prob(replicated_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cIUPgDtEeM8L",
    "outputId": "429200e7-e7d5-480e-bb74-8a5351602675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100, 2, 2)\n",
      "Original data\n",
      "[[ 1.9960071  1.371155 ]\n",
      " [-0.3108625 -0.8045084]\n",
      " [ 0.8756285  1.132521 ]\n",
      " [ 4.908268   2.111288 ]\n",
      " [-2.3772883 -1.4926063]]\n",
      "Replication 0\n",
      "[[ 1.9960071  1.371155 ]\n",
      " [-0.3108625 -0.8045084]\n",
      " [ 0.8756285  1.132521 ]\n",
      " [ 4.908268   2.111288 ]\n",
      " [-2.3772883 -1.4926063]]\n",
      "Replication 1\n",
      "[[ 1.9960071  1.371155 ]\n",
      " [-0.3108625 -0.8045084]\n",
      " [ 0.8756285  1.132521 ]\n",
      " [ 4.908268   2.111288 ]\n",
      " [-2.3772883 -1.4926063]]\n"
     ]
    }
   ],
   "source": [
    "# For our test, we'll use a tensor of 2 precision matrices.\n",
    "# We'll need to replicate our data for the likelihood function.\n",
    "# Remember, TFP wants the data to be structured so that the sample dimensions\n",
    "# are first (100 here), then the batch dimensions (2 here because we have 2\n",
    "# precision matrices), then the event dimensions (2 because we have 2-D\n",
    "# Gaussian data).  We'll need to add a middle dimension for the batch using\n",
    "# expand_dims, and then we'll need to create 2 replicates in this new dimension\n",
    "# using tile.\n",
    "n = 2\n",
    "print(np.shape(my_data))\n",
    "replicated_data = np.tile(np.expand_dims(my_data, axis=1), reps=[1, 2, 1])\n",
    "print (replicated_data.shape)\n",
    "\n",
    "# Sanity check\n",
    "print('Original data')\n",
    "print(my_data[0:5,:])\n",
    "print('Replication 0')\n",
    "print(replicated_data[0:5,0,:])\n",
    "print('Replication 1')\n",
    "print(replicated_data[0:5,1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFZ-bNCrgcDf"
   },
   "source": [
    "**Tip:**  One thing I've found to be extremely helpful is writing little sanity checks of my TensorFlow functions.  It's really easy to mess up the vectorization in TF, so having the simpler NumPy functions around is a great way to verify the TF output.  Think of these as little unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "msBv3Kn7J8Oy",
    "outputId": "e6366c0b-c293-4742-e274-6717d3f1e24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "numpy: -430.7121611327147\n",
      "tensorflow: -430.71204\n",
      "1\n",
      "numpy: -280.8182256375922\n",
      "tensorflow: -280.8182\n"
     ]
    }
   ],
   "source": [
    "# check against the numpy implementation\n",
    "with tf.Graph().as_default() as g:\n",
    "  precisions = np.stack([np.eye(2, dtype=np.float32), true_precision])\n",
    "  n = precisions.shape[0]\n",
    "  lik_tf = log_lik_data(precisions, replicated_data=replicated_data)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  lik_tf_ = sess.run(lik_tf)\n",
    "  for i in range(n):\n",
    "    print (i)\n",
    "    print ('numpy:', log_lik_data_numpy(precisions[i], my_data))\n",
    "    print ('tensorflow:', lik_tf_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y41-jXuMc0K-"
   },
   "source": [
    "### Prior log likelihood\n",
    "\n",
    "The prior is easier since we don't have to worry about data replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIzU4zNxEQPQ"
   },
   "outputs": [],
   "source": [
    "def log_lik_prior(precisions):\n",
    "  rv_precision = tfd.Wishart(\n",
    "      df=PRIOR_DF,\n",
    "      scale=PRIOR_SCALE,\n",
    "      validate_args=VALIDATE_ARGS,\n",
    "      allow_nan_stats=ALLOW_NAN_STATS)\n",
    "  return rv_precision.log_prob(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "e_RP9kLBdByx",
    "outputId": "4306b1da-15bb-4c72-c106-5c166f324435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "numpy: -2.2351873809649625\n",
      "tensorflow: -2.2351875\n",
      "1\n",
      "numpy: -9.103606346649766\n",
      "tensorflow: -9.103608\n"
     ]
    }
   ],
   "source": [
    "# check against the numpy implementation\n",
    "with tf.Graph().as_default() as g:\n",
    "  precisions = np.stack([np.eye(2, dtype=np.float32), true_precision])\n",
    "  n = precisions.shape[0]\n",
    "  lik_tf = log_lik_prior(precisions)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  lik_tf_ = sess.run(lik_tf)\n",
    "  for i in range(n):\n",
    "    print (i)\n",
    "    print ('numpy:', log_lik_prior_numpy(precisions[i]))\n",
    "    print ('tensorflow:', lik_tf_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwIARwvykZDZ"
   },
   "source": [
    "### Build the joint log likelihood function\n",
    "\n",
    "The data log likelihood function above depends on our observations, but the sampler won't have those.  We can get rid of the dependency without using a global variable by using a [closure](https://en.wikipedia.org/wiki/Closure_(computer_programming).  Closures involve an outer function that build an environment containing variables needed by an inner function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ps6teXnZluC5"
   },
   "outputs": [],
   "source": [
    "def get_log_lik(data, n_chains=1):\n",
    "  # The data argument that is passed in will be available to the inner function\n",
    "  # below so it doesn't have to be passed in as a parameter.\n",
    "  replicated_data = np.tile(np.expand_dims(data, axis=1), reps=[1, n_chains, 1])\n",
    "\n",
    "  def _log_lik(precision):\n",
    "    return log_lik_data(precision, replicated_data) + log_lik_prior(precision)\n",
    "\n",
    "  return _log_lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "1ThOyNvXkW6m",
    "outputId": "f44e668d-294a-42cf-9f34-259a08b1b38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cholesky decomposition was not successful. The input might not be valid.\n",
      "\t [[node mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/Wishart/log_prob/Cholesky (defined at /home/akshaya/anaconda3/envs/skg-tensorflow/lib/python3.6/site-packages/tensorflow_probability/python/distributions/wishart.py:275)  = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/add_1)]]\n",
      "\n",
      "Caused by op 'mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/Wishart/log_prob/Cholesky', defined at:\n",
      "  File \"/home/akshaya/anaconda3/envs/skg-tensorflow/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "...\n",
      "InvalidArgumentError (see above for traceback): Cholesky decomposition was not successful. The input might not be valid.\n",
      "\t [[node mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/Wishart/log_prob/Cholesky (defined at /home/akshaya/anaconda3/envs/skg-tensorflow/lib/python3.6/site-packages/tensorflow_probability/python/distributions/wishart.py:275)  = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/add_1)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "  # Use expand_dims because we want to pass in a tensor of starting values\n",
    "  init_precision = tf.expand_dims(tf.eye(2), axis=0)\n",
    "  log_lik_fn = get_log_lik(my_data, n_chains=1)\n",
    "\n",
    "  # we'll just do a few steps here\n",
    "  num_results = 10\n",
    "  num_burnin_steps = 10\n",
    "\n",
    "  states, kernel_results = tfp.mcmc.sample_chain(\n",
    "      num_results=num_results,\n",
    "      num_burnin_steps=num_burnin_steps,\n",
    "      current_state=[\n",
    "          init_precision,\n",
    "      ],\n",
    "      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "          target_log_prob_fn=log_lik_fn,\n",
    "          step_size=0.1,\n",
    "          num_leapfrog_steps=3,\n",
    "          seed=123),\n",
    "      parallel_iterations=1)\n",
    "  \n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  tf.set_random_seed(123)\n",
    "\n",
    "  try:\n",
    "    states_, kernel_results_ = sess.run([states, kernel_results])\n",
    "  except Exception as e:\n",
    "    # shorten the giant stack trace\n",
    "    lines = str(e).split('\\n')\n",
    "    print('\\n'.join(lines[:5]+['...']+lines[-3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "je9NrE27snQG"
   },
   "source": [
    "### Identifying the problem\n",
    "\n",
    "`InvalidArgumentError (see above for traceback): Cholesky decomposition was not successful. The input might not be valid.`  That's not super helpful.  Let's see if we can find out more about what happened.\n",
    "\n",
    "* We'll print out the parameters for each step so we can see the value for which things fail\n",
    "* We'll add some assertions to guard against specific problems.\n",
    "\n",
    "Assertions are tricky because they're TensorFlow operations, and we have to take care that they get executed and don't get optimized out of the graph.  It's worth reading [this overview](https://wookayin.github.io/tensorflow-talk-debugging/#1) of TensorFlow debugging if you aren't familiar with TF assertions.  You can explicitly force assertions to execute using `tf.control_dependencies` (see the comments in the code below).\n",
    "\n",
    "TensorFlow's native `Print` function has the same behavior as assertions - it's an operation, and you need to take some care to ensure that it executes.  `Print` causes additional headaches when we're working in a notebook: its output is sent to `stderr`, and `stderr` isn't displayed in the cell.  We'll use a trick here: instead of using `tf.Print`, we'll create our own TensorFlow print operation via `tf.pyfunc`.  As with assertions, we have to make sure our method executes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWg3y5KU_mg9"
   },
   "outputs": [],
   "source": [
    "def get_log_lik_verbose(data, n_chains=1):\n",
    "  # The data argument that is passed in will be available to the inner function\n",
    "  # below so it doesn't have to be passed in as a parameter.\n",
    "  replicated_data = np.tile(np.expand_dims(data, axis=1), reps=[1, n_chains, 1])\n",
    "\n",
    "  def _log_lik(precisions):\n",
    "    # An internal method we'll make into a TensorFlow operation via tf.py_func\n",
    "    def _print_precisions(precisions):\n",
    "      print ('precisions:\\n', precisions)\n",
    "      return False  # operations must return something!\n",
    "    # Turn our method into a TensorFlow operation\n",
    "    print_op = tf.py_func(_print_precisions, [precisions], tf.bool)\n",
    "\n",
    "    # Assertions are also operations, and some care needs to be taken to ensure\n",
    "    # that they're executed\n",
    "    assert_op = tf.assert_equal(\n",
    "        precisions, tf.transpose(precisions, perm=[0, 2, 1]), data=[precisions],\n",
    "        message='not symmetrical', summarize=4, name='symmetry_check')\n",
    "\n",
    "    # The control_dependencies statement forces its arguments to be executed\n",
    "    # before subsequent operations\n",
    "    with tf.control_dependencies([print_op, assert_op]):\n",
    "      return (log_lik_data(precisions, replicated_data) +\n",
    "              log_lik_prior(precisions))\n",
    "\n",
    "  return _log_lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "znG_AtTR7qob",
    "outputId": "ccae3fb5-c5c6-4f0b-bff2-556bbcabdade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisions:\n",
      " [[[1. 0.]\n",
      "  [0. 1.]]]\n",
      "precisions:\n",
      " [[[ 0.17633396 -0.43482423]\n",
      "  [-0.48801905  1.0519004 ]]]\n",
      "assertion failed: [[[0.176333964 -0.434824228][-0.488019049 1.05190039]]]\n",
      "\t [[node mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/symmetry_check/Assert/AssertGuard/Assert (defined at <ipython-input-33-cd58a2918834>:18)  = Assert[T=[DT_FLOAT], summarize=4, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/symmetry_check/Assert/AssertGuard/Assert/Switch, mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/symmetry_check/Assert/AssertGuard/Assert/Switch_1)]]\n",
      "\n",
      "Caused by op 'mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/symmetry_check/Assert/AssertGuard/Assert', defined at:\n",
      "  File \"/home/akshaya/anaconda3/envs/skg-tensorflow/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "...\n",
      "InvalidArgumentError (see above for traceback): assertion failed: [[[0.176333964 -0.434824228][-0.488019049 1.05190039]]]\n",
      "\t [[node mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/symmetry_check/Assert/AssertGuard/Assert (defined at <ipython-input-33-cd58a2918834>:18)  = Assert[T=[DT_FLOAT], summarize=4, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/symmetry_check/Assert/AssertGuard/Assert/Switch, mcmc_sample_chain/scan/while/smart_for_loop/while/mh_one_step/hmc_kernel_one_step/while/hmc_leapfrog_integrator_one_step/maybe_call_fn_and_grads/value_and_gradients/symmetry_check/Assert/AssertGuard/Assert/Switch_1)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "  tf.set_random_seed(123)\n",
    "  init_precision = tf.expand_dims(tf.eye(2), axis=0)\n",
    "  log_lik_fn = get_log_lik_verbose(my_data)\n",
    "\n",
    "  # we'll just do a few steps here\n",
    "  num_results = 10\n",
    "  num_burnin_steps = 10\n",
    "\n",
    "  states, kernel_results = tfp.mcmc.sample_chain(\n",
    "      num_results=num_results,\n",
    "      num_burnin_steps=num_burnin_steps,\n",
    "      current_state=[\n",
    "          init_precision,\n",
    "      ],\n",
    "      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "          target_log_prob_fn=log_lik_fn,\n",
    "          step_size=0.1,\n",
    "          num_leapfrog_steps=3,\n",
    "          seed=123),\n",
    "      parallel_iterations=1)\n",
    "  \n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  try:\n",
    "    states_, kernel_results_ = sess.run([states, kernel_results])\n",
    "  except Exception as e:\n",
    "    # shorten the giant stack trace\n",
    "    lines = str(e).split('\\n')\n",
    "    print ('\\n'.join(lines[:5]+['...']+lines[-3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPBlB7expwuw"
   },
   "source": [
    "### Why this fails\n",
    "\n",
    "The very first new parameter value the sampler tries is an asymmetrical matrix.  That causes the Cholesky decomposition to fail, since it's only defined for symmetrical (and positive definite) matrices.\n",
    "\n",
    "The problem here is that our parameter of interest is a precision matrix, and precision matrices must be real, symmetric, and positive definite.  The sampler doesn't know anything about this constraint (except possibly through gradients), so it is entirely possible that the sampler will propose an invalid value, leading to an exception, particularly if the step size is large.\n",
    "\n",
    "With the Hamiltonian Monte Carlo sampler, we may be able to work around the problem by using a very small step size, since the gradient should keep the parameters away from invalid regions, but small step sizes mean slow convergence.  With a Metropolis-Hastings sampler, which doesn't know anything about gradients, we're doomed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9RmouuKCsaYv"
   },
   "source": [
    "# Version 2: reparametrizing to unconstrained parameters\n",
    "\n",
    "There is a straightforward solution to the problem above: we can reparametrize our model such that the new parameters no longer have these constraints.  TFP provides a useful set of tools - bijectors - for doing just that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGbGGMRs01Sk"
   },
   "source": [
    "### Reparameterization with bijectors\n",
    "\n",
    "Our precision matrix must be real and symmetric; we want an alternative parameterization that doesn't have these constraints.  A starting point is a Cholesky factorization of the precision matrix.  The Cholesky factors are still constrained - they are lower triangular, and their diagonal elements must be positive.  However, if we take the log of the diagonals of the Cholesky factor, the logs are no longer are constrained to be positive, and then if we flatten the lower triangular portion into a 1-D vector, we no longer have the lower triangular constraint.  The result in our case will be a length 3 vector with no constraints.\n",
    "\n",
    "(The [Stan manual](http://mc-stan.org/users/documentation/) has a great chapter on using transformations to remove various types of constraints on parameters.)\n",
    "\n",
    "This reparameterization has little effect on our data log likelihood function - we just have to invert our transformation so we get back the precision matrix - but the effect on the prior is more complicated.  We've specified that the probability of a given precision matrix is given by the Wishart distribution; what is the probability of our transformed matrix?\n",
    "\n",
    "Recall that if we apply a monotonic function $g$ to a 1-D random variable $X$, $Y = g(X)$, the density for $Y$ is given by \n",
    "\n",
    "$$\n",
    "f_Y(y) = | \\frac{d}{dy}(g^{-1}(y)) | f_X(g^{-1}(y))\n",
    "$$\n",
    "\n",
    "The derivative of $g^{-1}$ term accounts for the way that $g$ changes local volumes.  For higher dimensional random variables, the corrective factor is the absolute value of the determinant of the Jacobian of $g^{-1}$ (see [here](https://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables)).\n",
    "\n",
    "We'll have to add a Jacobian of the inverse transform into our log prior likelihood function.  Happily, TFP's `Bijector` class can take care of this for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8deaPaGI6MZ"
   },
   "source": [
    "The [`Bijector`](https://www.tensorflow.org/api_docs/python/tf/distributions/bijectors/Bijector) class is used to represent invertible, smooth functions used for changing variables in probability density functions.  Bijectors all have a `forward()` method that performs a transform, an `inverse()` method that inverts it, and `forward_log_det_jacobian()` and `inverse_log_det_jacobian()` methods that provide the Jacobian corrections we need when we reparaterize a pdf.\n",
    "\n",
    "TFP provides a collection of useful bijectors that we can combine through composition via the [`Chain`](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Chain) operator to form quite complicated transforms.  In our case, we'll compose the following 3 bijectors (the operations in the chain are performed from right to left):\n",
    "\n",
    "1. The first step of our transform is to perform a Cholesky factorization on the precision matrix.  There isn't a Bijector class for that; however, the [`CholeskyOuterProduct`](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/CholeskyOuterProduct) bijector takes the product of 2 Cholesky factors.  We can use the inverse of that operation using the [`Invert`](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Invert) operator.\n",
    "2. The next step is to take the log of the diagonal elements of the Cholesky factor.  We accomplish this via the `TransformDiagonal` bijector and the inverse of the [`Exp`](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Exp) bijector.\n",
    "3. Finally we flatten the lower triangular portion of the matrix to a vector using the inverse of the `FillTriangular` bijector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OM4s01mGsjfZ"
   },
   "outputs": [],
   "source": [
    "# Our transform has 3 stages that we chain together via composition:\n",
    "precision_to_unconstrained = tfb.Chain([\n",
    "    # step 3: flatten the lower triangular portion of the matrix\n",
    "    tfb.Invert(tfb.FillTriangular(validate_args=VALIDATE_ARGS)),\n",
    "    # step 2: take the log of the diagonals    \n",
    "    tfb.TransformDiagonal(tfb.Invert(tfb.Exp(validate_args=VALIDATE_ARGS))),\n",
    "    # step 1: decompose the precision matrix into its Cholesky factors\n",
    "    tfb.Invert(tfb.CholeskyOuterProduct(validate_args=VALIDATE_ARGS)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "z7qIfflyn6Bv",
    "outputId": "44c1b468-1f06-4d49-878b-dbb090fa18af"
   },
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "with tf.Graph().as_default() as g:\n",
    "  m = tf.constant([[1., 2.], [2., 8.]])\n",
    "  m_fwd = precision_to_unconstrained.forward(m)\n",
    "  m_inv = precision_to_unconstrained.inverse(m_fwd)\n",
    "\n",
    "  # bijectors handle tensors of values, too!\n",
    "  m2 = tf.stack([m, tf.eye(2)])\n",
    "  m2_fwd = precision_to_unconstrained.forward(m2)\n",
    "  m2_inv = precision_to_unconstrained.inverse(m2_fwd)\n",
    "  \n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  m_, m_fwd_, m_inv_ = sess.run([m, m_fwd, m_inv])\n",
    "  print 'single input:'\n",
    "  print 'm:\\n', m_\n",
    "  print 'precision_to_unconstrained(m):\\n', m_fwd_\n",
    "  print 'inverse(precision_to_unconstrained(m)):\\n', m_inv_\n",
    "  print\n",
    "\n",
    "  m2_, m2_fwd_, m2_inv_ = sess.run([m2, m2_fwd, m2_inv])\n",
    "  print 'tensor of inputs:'\n",
    "  print 'm2:\\n', m2_\n",
    "  print 'precision_to_unconstrained(m2):\\n', m2_fwd_\n",
    "  print 'inverse(precision_to_unconstrained(m2)):\\n', m2_inv_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wVq7AwOKYhd"
   },
   "source": [
    "The [`TransformedDistribution`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/TransformedDistribution) class automates the process of applying a bijector to a distribution and making the necessary Jacobian correction to `log_prob()`.  Our new prior becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0pQ7HqrN8aq"
   },
   "outputs": [],
   "source": [
    "def log_lik_prior_transformed(transformed_precisions):\n",
    "  rv_precision = tfd.TransformedDistribution(\n",
    "      tfd.Wishart(\n",
    "          df=PRIOR_DF,\n",
    "          scale=PRIOR_SCALE,\n",
    "          validate_args=VALIDATE_ARGS,\n",
    "          allow_nan_stats=ALLOW_NAN_STATS),\n",
    "      bijector=precision_to_unconstrained,\n",
    "      validate_args=VALIDATE_ARGS)\n",
    "  return rv_precision.log_prob(transformed_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "-ddk6nKoO2tv",
    "outputId": "318f4700-a652-462e-f1fa-8b7f6a24ff8b"
   },
   "outputs": [],
   "source": [
    "# Check against the numpy implementation.  Note that when comparing, we need\n",
    "# to add in the Jacobian correction.\n",
    "with tf.Graph().as_default() as g:\n",
    "  precisions = np.stack([np.eye(2, dtype=np.float32), true_precision])\n",
    "  transformed_precisions = precision_to_unconstrained.forward(precisions)\n",
    "  lik_tf = log_lik_prior_transformed(transformed_precisions)\n",
    "  corrections = precision_to_unconstrained.inverse_log_det_jacobian(\n",
    "      transformed_precisions, event_ndims=1)\n",
    "  n = precisions.shape[0]\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  lik_tf_, corrections_ = sess.run([lik_tf, corrections])\n",
    "  for i in range(n):\n",
    "    print i\n",
    "    print 'numpy:', log_lik_prior_numpy(precisions[i]) + corrections_[i]\n",
    "    print 'tensorflow:', lik_tf_[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaSA9l_UQq9Y"
   },
   "source": [
    "We just need to invert the transform for our data log likelihood:\n",
    "\n",
    "`precision = precision_to_unconstrained.inverse(transformed_precision)\n",
    "`\n",
    "\n",
    "Since we actually want the Cholesky factorization of the precision matrix, it would be more efficient to do just a partial inverse here.  However, we'll leave optimization for later and will leave the partial inverse as an exercise for the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vM-nF4t2QqSr"
   },
   "outputs": [],
   "source": [
    "def log_lik_data_transformed(transformed_precisions, replicated_data):\n",
    "  # We recover the precision matrix by inverting our bijector.  This is\n",
    "  # inefficient since we really want the Cholesky decomposition of the\n",
    "  # precision matrix, and the bijector has that in hand during the inversion,\n",
    "  # but we'll worry about efficiency later.\n",
    "  n = tf.shape(transformed_precisions)[0]\n",
    "  precisions = precision_to_unconstrained.inverse(transformed_precisions)\n",
    "  precisions_cholesky = tf.cholesky(precisions)\n",
    "  covariances = tf.cholesky_solve(precisions_cholesky,\n",
    "                                  tf.eye(2, batch_shape=[n]))\n",
    "  rv_data = tfd.MultivariateNormalFullCovariance(\n",
    "      loc=tf.zeros([n, 2]),\n",
    "      covariance_matrix=covariances,\n",
    "      validate_args=VALIDATE_ARGS,\n",
    "      allow_nan_stats=ALLOW_NAN_STATS)\n",
    "\n",
    "  return tf.reduce_sum(rv_data.log_prob(replicated_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "WyoYF8AM3d1q",
    "outputId": "3ced3835-dd51-4e9a-f70a-47dc606845c5"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "with tf.Graph().as_default() as g:\n",
    "  precisions = np.stack([np.eye(2, dtype=np.float32), true_precision])\n",
    "  transformed_precisions = precision_to_unconstrained.forward(precisions)\n",
    "  lik_tf = log_lik_data_transformed(transformed_precisions, replicated_data)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  lik_tf_ = sess.run(lik_tf)\n",
    "  for i in range(precisions.shape[0]):\n",
    "    print i\n",
    "    print 'numpy:', log_lik_data_numpy(precisions[i], my_data)\n",
    "    print 'tensorflow:', lik_tf_[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3w3sMPgITLRM"
   },
   "source": [
    "Again we wrap our new functions in a closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKWHJFisTIzo"
   },
   "outputs": [],
   "source": [
    "def get_log_lik_transformed(data, n_chains=1):\n",
    "  # The data argument that is passed in will be available to the inner function\n",
    "  # below so it doesn't have to be passed in as a parameter.\n",
    "  replicated_data = np.tile(np.expand_dims(data, axis=1), reps=[1, n_chains, 1])\n",
    "\n",
    "  def _log_lik_transformed(transformed_precisions):\n",
    "    return (log_lik_data_transformed(transformed_precisions, replicated_data) +\n",
    "            log_lik_prior_transformed(transformed_precisions))\n",
    "\n",
    "  return _log_lik_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Uu1eJSOqiZ3S",
    "outputId": "d755cd8d-9d4b-4355-ba60-341e4c0bd7f2"
   },
   "outputs": [],
   "source": [
    "# make sure everything runs\n",
    "with tf.Graph().as_default() as g:\n",
    "  log_lik_fn = get_log_lik_transformed(my_data)\n",
    "  m = tf.expand_dims(tf.eye(2), axis=0)\n",
    "  lik = log_lik_fn(precision_to_unconstrained.forward(m))\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  print sess.run(lik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-XRDaXcTmX8"
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that we don't have to worry about our sampler blowing up because of invalid parameter values, let's generate some real samples.\n",
    "\n",
    "The sampler works with the unconstrained version of our parameters, so we need to transform our initial value to its unconstrained version.  The samples that we generate will also all be in their unconstrained form, so we need to transform them back.  Bijectors are vectorized, so it's easy to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFvyLlP_Tbi4"
   },
   "outputs": [],
   "source": [
    "# We'll choose a proper random initial value this time\n",
    "np.random.seed(123)\n",
    "initial_value_cholesky = np.array(\n",
    "    [[0.5 + np.random.uniform(), 0.0],\n",
    "     [-0.5 + np.random.uniform(), 0.5 + np.random.uniform()]],\n",
    "    dtype=np.float32)\n",
    "initial_value = np.expand_dims(\n",
    "    initial_value_cholesky.dot(initial_value_cholesky.T), axis=0)\n",
    "\n",
    "# The sampler works with unconstrained values, so we'll transform our initial\n",
    "# value\n",
    "with tf.Graph().as_default() as g:\n",
    "  initial_value_transformed = precision_to_unconstrained.forward(initial_value)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  initial_value_transformed_ = sess.run(initial_value_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUobCu7xTnoa"
   },
   "outputs": [],
   "source": [
    "# Sample!\n",
    "with tf.Graph().as_default() as g:\n",
    "  tf.set_random_seed(123)\n",
    "  log_lik_fn = get_log_lik_transformed(my_data, n_chains=1)\n",
    "\n",
    "  num_results = 1000\n",
    "  num_burnin_steps = 1000\n",
    "\n",
    "  states, kernel_results = tfp.mcmc.sample_chain(\n",
    "      num_results=num_results,\n",
    "      num_burnin_steps=num_burnin_steps,\n",
    "      current_state=[\n",
    "          initial_value_transformed_,\n",
    "      ],\n",
    "      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "          target_log_prob_fn=log_lik_fn,\n",
    "          step_size=0.1,\n",
    "          num_leapfrog_steps=3,\n",
    "          seed=123),\n",
    "      parallel_iterations=1)\n",
    "  # transform samples back to their constrained form\n",
    "  precision_samples = precision_to_unconstrained.inverse(states)\n",
    "  \n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  states_, precision_samples_, kernel_results_ = sess.run(\n",
    "      [states, precision_samples, kernel_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-klSjK2m5mM"
   },
   "source": [
    "Let's compare the mean of our sampler's output to the analytic posterior mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Xw7VHCMvkPKg",
    "outputId": "595e8f47-77e9-457d-9aab-e96e1625cc9b"
   },
   "outputs": [],
   "source": [
    "print 'True posterior mean:\\n', posterior_mean\n",
    "print 'Sample mean:\\n', np.mean(np.reshape(precision_samples_, [-1, 2, 2]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXvQbe7JndV-"
   },
   "source": [
    "We're way off!  Let's figure out why.  First let's look at our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "ajQSpMwfnmHr",
    "outputId": "c3ff41b6-417c-4a9a-a13b-0c407f8d03e8"
   },
   "outputs": [],
   "source": [
    "np.reshape(precision_samples_, [-1, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayxqUbCZnu-l"
   },
   "source": [
    "Uh oh - it looks like they all have the same value.  Let's figure out why.\n",
    "\n",
    "The `kernel_results_` variable is a named tuple that gives information about the sampler at each state.  The `is_accepted` field is the key here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "grRrCz0kn4D1",
    "outputId": "e41ebfb4-fe0d-48a9-da05-793694e18b11"
   },
   "outputs": [],
   "source": [
    "# Look at the acceptance for the last 100 samples\n",
    "print np.squeeze(kernel_results_.is_accepted)[-100:]\n",
    "print 'Fraction of samples accepted:', np.mean(np.squeeze(kernel_results_.is_accepted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cCibOo-rOG-"
   },
   "source": [
    "All our samples were rejected!  Presumably our step size was too big.  I chose `stepsize=0.1` purely arbitrarily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "doCMVXNReOBZ"
   },
   "source": [
    "# Version 3: sampling with an adaptive step size\n",
    "\n",
    "Since sampling with my arbitrary choice of step size failed, we have a few agenda items:\n",
    "1. implement an adaptive step size, and\n",
    "2. perform some convergence checks.\n",
    "\n",
    "There is some nice sample code in `tensorflow_probability/python/mcmc/hmc.py` for implementing adaptive step sizes.  I've adapted it below.\n",
    "\n",
    "Note that there's a separate `sess.run()` statement for each step.  This is really helpful for debugging, since it allows us to easily add some per-step diagnostics if need be.  For example, we can show incremental progress, time each step, etc.\n",
    "\n",
    "**Tip:** One apparently common way to mess up your sampling is to have your graph grow in the loop.  (The reason for finalizing the graph before the session is run is to prevent just such problems.)  If you haven't been using finalize(), though, a useful debugging check if your code slows to a crawl is to print out the graph size at each step via `len(mygraph.get_operations())` - if the length increases, you're probably doing something bad.\n",
    "\n",
    "We're going to run 3 independent chains here.  Doing some comparisons between the chains will help us check for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgLX6o9PZRwQ"
   },
   "outputs": [],
   "source": [
    "# The number of chains is determined by the shape of the initial values.\n",
    "# Here we'll generate 3 chains, so we'll need a tensor of 3 initial values.\n",
    "N_CHAINS = 3\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "initial_values = []\n",
    "for i in range(N_CHAINS):\n",
    "  initial_value_cholesky = np.array(\n",
    "      [[0.5 + np.random.uniform(), 0.0],\n",
    "       [-0.5 + np.random.uniform(), 0.5 + np.random.uniform()]],\n",
    "      dtype=np.float32)\n",
    "  initial_values.append(initial_value_cholesky.dot(initial_value_cholesky.T))\n",
    "initial_values = np.stack(initial_values)\n",
    "\n",
    "# Transform our initial values to their unconstrained form\n",
    "# (Transforming the value in its own session is a workaround for b/72831017)\n",
    "with tf.Graph().as_default() as g:\n",
    "  initial_values_transformed = precision_to_unconstrained.forward(\n",
    "      initial_values)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  initial_values_transformed_ = sess.run(initial_values_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "M7A-JG6hwCVu",
    "outputId": "924cb395-2242-43d6-c7f8-d43c3db1df73"
   },
   "outputs": [],
   "source": [
    "# Code adapted from tensorflow_probability/python/mcmc/hmc.py\n",
    "with tf.Graph().as_default() as g:\n",
    "  tf.set_random_seed(123)\n",
    "  log_lik_fn = get_log_lik_transformed(my_data)\n",
    "\n",
    "  # Tuning acceptance rates:\n",
    "  dtype = np.float32\n",
    "  num_warmup_iter = 2500\n",
    "  num_chain_iter = 2500\n",
    "\n",
    "  # Set the target average acceptance ratio for the HMC as suggested by\n",
    "  # Beskos et al. (2013):\n",
    "  # https://projecteuclid.org/download/pdfview_1/euclid.bj/1383661192\n",
    "  target_accept_rate = 0.651\n",
    "\n",
    "  x = tf.get_variable(name='x', initializer=initial_values_transformed_)\n",
    "  step_size = tf.get_variable(name='step_size',\n",
    "                              initializer=tf.constant(0.01, dtype=dtype))\n",
    "\n",
    "  # Initialize the HMC sampler.\n",
    "  hmc = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "      target_log_prob_fn=log_lik_fn,\n",
    "      step_size=step_size,\n",
    "      num_leapfrog_steps=3)\n",
    "\n",
    "  # One iteration of the HMC\n",
    "  next_x, other_results = hmc.one_step(\n",
    "      current_state=x,\n",
    "      previous_kernel_results=hmc.bootstrap_results(x))\n",
    "\n",
    "  x_update = x.assign(next_x)\n",
    "  precision = precision_to_unconstrained.inverse(x_update)\n",
    "\n",
    "  # Adapt the step size using standard adaptive MCMC procedure. See Section 4.2\n",
    "  # of Andrieu and Thoms (2008):\n",
    "  # http://www4.ncsu.edu/~rsmith/MA797V_S12/Andrieu08_AdaptiveMCMC_Tutorial.pdf\n",
    "\n",
    "  # NOTE: One important change we need to make from the hmc.py version is to\n",
    "  # combine the log_accept_ratio values from the different chains when\n",
    "  # deciding how to update the step size.  Here we use the mean\n",
    "  # log_accept_ratio to decide.\n",
    "  step_size_update = step_size.assign_add(\n",
    "      step_size * tf.where(\n",
    "          tf.exp(tf.minimum(tf.reduce_mean(\n",
    "              other_results.log_accept_ratio), 0.)) >\n",
    "              target_accept_rate,\n",
    "          x=0.1, y=-0.1))\n",
    "\n",
    "  # Note, the adaptations are performed during warmup only.\n",
    "  warmup = tf.group([x_update, step_size_update])\n",
    "\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  # Initialize variables\n",
    "  sess.run(init)\n",
    "\n",
    "  # Warm up the sampler and adapt the step size\n",
    "  print 'Warmup'\n",
    "  start_time = time.time()\n",
    "  for i in range(num_warmup_iter):\n",
    "    sess.run(warmup)\n",
    "    if i % 500 == 0:\n",
    "      print 'Step %d' % i\n",
    "  end_time = time.time()\n",
    "  print 'Time per step:', (end_time - start_time) / num_warmup_iter\n",
    "  print 'Step size: %g' % sess.run(step_size)\n",
    "\n",
    "  # Collect samples without adapting step size\n",
    "  print 'Sampling'\n",
    "  start_time = time.time()\n",
    "  packed_samples = np.zeros([num_chain_iter, N_CHAINS, 3])\n",
    "  precision_samples = np.zeros([num_chain_iter, N_CHAINS, 2, 2])\n",
    "  results = []\n",
    "  for i in range(num_chain_iter):\n",
    "    _, x_, precision_, other_results_ = sess.run(\n",
    "        [x_update, x, precision, other_results])\n",
    "    packed_samples[i, :] = x_\n",
    "    precision_samples[i, :] = precision_\n",
    "    results.append(other_results_)\n",
    "    if i % 500 == 0:\n",
    "      print 'Step %d' % i\n",
    "  end_time = time.time()\n",
    "  print 'Time per step:', (end_time - start_time) / num_chain_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AITl0hs2KmHZ"
   },
   "source": [
    "A quick check: our acceptance rate during our sampling is close to our target of 0.651."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cFE1S3FX0alA",
    "outputId": "4a5c539b-ef79-497a-c381-977703ce7b03"
   },
   "outputs": [],
   "source": [
    "is_accepted = np.array([r.is_accepted for r in results])\n",
    "print np.mean(is_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lk4qcX4pz3pA"
   },
   "outputs": [],
   "source": [
    "precision_samples_reshaped = np.reshape(precision_samples, [-1, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPWKLqVOKuRC"
   },
   "source": [
    "Even better, our sample mean and standard deviation are close to what we expect from the analytic solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ktZJl5c01dmL",
    "outputId": "72d45460-91c6-4942-9274-5c8547955bd4"
   },
   "outputs": [],
   "source": [
    "print 'True posterior mean:\\n', posterior_mean\n",
    "print 'Mean of samples:\\n', np.mean(precision_samples_reshaped, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "cmPzXSn83yYa",
    "outputId": "f82a9d3c-2e76-4570-8986-047977359174"
   },
   "outputs": [],
   "source": [
    "print 'True posterior standard deviation:\\n', posterior_sd\n",
    "print 'Standard deviation of samples:\\n', np.std(precision_samples_reshaped, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYrX5WylYdIx"
   },
   "source": [
    "## Checking for convergence\n",
    "\n",
    "In general we won't have an analytic solution to check against, so we'll need to make sure the sampler has converged.  One standard check is the Gelman-Rubin $\\hat{R}$ statistic, which requires multiple sampling chains.  $\\hat{R}$ measures the degree to which variance (of the means) between chains exceeds what one would expect if the chains were identically distributed.  Values of $\\hat{R}$ close to 1 are used to indicate approximate convergence.  See [the source](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/mcmc/diagnostic.py#L205) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rfQXeVn-77yD",
    "outputId": "cb54f4f5-f73b-4f98-9a11-f1aab927c7f1"
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "  r_hat = tfp.mcmc.potential_scale_reduction(precision_samples)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  print sess.run(r_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKCsWnBkpaTM"
   },
   "source": [
    "## Model criticism\n",
    "\n",
    "If we didn't have an analytic solution, this would be the time to do some real model criticism.\n",
    "\n",
    "Here are a few quick histograms of the sample components relative to our ground truth (in red).  Note that the samples have been shrunk from the sample precision matrix values toward the identity matrix prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "oOTvOJBmpqhS",
    "outputId": "a75cf981-d137-407b-c41c-d57a9551120a"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, sharey=True)\n",
    "fig.set_size_inches(8, 8)\n",
    "for i in range(2):\n",
    "  for j in range(2):\n",
    "    ax = axes[i, j]\n",
    "    ax.hist(precision_samples_reshaped[:, i, j])\n",
    "    ax.axvline(true_precision[i, j], color='red',\n",
    "               label='True precision')\n",
    "    ax.axvline(sample_precision[i, j], color='red', linestyle=':',\n",
    "               label='Sample precision')\n",
    "    ax.set_title('precision[%d, %d]' % (i, j))\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Oi6M2ZmvKVL"
   },
   "source": [
    "Some scatterplots of pairs of precision components show that because of the correlation structure of the posterior, the true posterior values are not as unlikely as they appear from the marginals above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 869
    },
    "colab_type": "code",
    "id": "5vT6MyKVslqV",
    "outputId": "cb79b048-ba53-4cd5-9812-a562596c8e61"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4)\n",
    "fig.set_size_inches(12, 12)\n",
    "for i1 in range(2):\n",
    "  for j1 in range(2):\n",
    "    index1 = 2 * i1 + j1\n",
    "    for i2 in range(2):\n",
    "      for j2 in range(2):\n",
    "        index2 = 2 * i2 + j2\n",
    "        ax = axes[index1, index2]\n",
    "        ax.scatter(precision_samples_reshaped[:, i1, j1],\n",
    "                   precision_samples_reshaped[:, i2, j2], alpha=0.1)\n",
    "        ax.axvline(true_precision[i1, j1], color='red')\n",
    "        ax.axhline(true_precision[i2, j2], color='red')\n",
    "        ax.axvline(sample_precision[i1, j1], color='red', linestyle=':')\n",
    "        ax.axhline(sample_precision[i2, j2], color='red', linestyle=':')\n",
    "        ax.set_title('(%d, %d) vs (%d, %d)' % (i1, j1, i2, j2))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcmvbpxoMYtl"
   },
   "source": [
    "# Version 4: simpler sampling of constrained parameters\n",
    "\n",
    "Bijectors made sampling the precision matrix straightforward, but there was a fair amount of manual converting to and from the unconstrained representation.  There is an easier way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mv2bgnmVvfxk"
   },
   "source": [
    "### The TransformedTransitionKernel\n",
    "\n",
    "The `TransformedTransitionKernel` simplifies this process.  It wraps your sampler and handles all the conversions.  It takes as an argument a list of bijectors that map unconstrained parameter values to constrained ones.  So here we need the inverse of the `precision_to_unconstrained` bijector we used above.  We could just use `tfb.Invert(precision_to_unconstrained)`, but that would involve taking of inverses of inverses (TensorFlow isn't smart enough to simplify `tf.Invert(tf.Invert())` to `tf.Identity())`, so instead we'll just write a new bijector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paGZdKlRvj7E"
   },
   "source": [
    "### Constraining bijector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vv4JqbHUP9n7"
   },
   "outputs": [],
   "source": [
    "# The bijector we need for the TransformedTransitionKernel is the inverse of\n",
    "# the one we used above\n",
    "unconstrained_to_precision = tfb.Chain([\n",
    "    # step 3: take the product of Cholesky factors\n",
    "    tfb.CholeskyOuterProduct(validate_args=VALIDATE_ARGS),\n",
    "    # step 2: exponentiate the diagonals    \n",
    "    tfb.TransformDiagonal(tfb.Exp(validate_args=VALIDATE_ARGS)),\n",
    "    # step 3: map a vector to a lower triangular matrix\n",
    "    tfb.FillTriangular(validate_args=VALIDATE_ARGS),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "VWgW2Qk0eM8L",
    "outputId": "00f709b9-2814-4870-aaab-d3ac2c7291f1"
   },
   "outputs": [],
   "source": [
    "# quick sanity check\n",
    "with tf.Graph().as_default() as g:\n",
    "  m = tf.constant([[1., 2.], [2., 8.]])\n",
    "  m_inv = unconstrained_to_precision.inverse(m)\n",
    "  m_fwd = unconstrained_to_precision.forward(m_inv)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  m_, m_inv_, m_fwd_ = sess.run([m, m_inv, m_fwd])\n",
    "  print 'm:\\n', m_\n",
    "  print 'unconstrained_to_precision.inverse(m):\\n', m_inv_\n",
    "  print 'forward(unconstrained_to_precision.inverse(m)):\\n', m_fwd_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wnVsQFysvoM2"
   },
   "source": [
    "## Sampling with the TransformedTransitionKernel\n",
    "\n",
    "With the `TransformedTransitionKernel`, we no longer have to do manual transformations of our parameters.  Our initial values and our samples are all precision matrices; we just have to pass in our unconstraining bijector(s) to the kernel and it takes care of all the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "a2VVVg4KhSnb",
    "outputId": "a498d3b1-208c-4170-e5e9-2007e959e290"
   },
   "outputs": [],
   "source": [
    "# Code adapted from tensorflow_probability/python/mcmc/hmc.py\n",
    "with tf.Graph().as_default() as g:\n",
    "  tf.set_random_seed(123)\n",
    "  log_lik_fn = get_log_lik(my_data)\n",
    "\n",
    "  # Tuning acceptance rates:\n",
    "  dtype = np.float32\n",
    "  num_warmup_iter = 2500\n",
    "  num_chain_iter = 2500\n",
    "\n",
    "  # Set the target average acceptance ratio for the HMC as suggested by\n",
    "  # Beskos et al. (2013):\n",
    "  # https://projecteuclid.org/download/pdfview_1/euclid.bj/1383661192\n",
    "  target_accept_rate = 0.651\n",
    "\n",
    "  x = tf.get_variable(name='x', initializer=initial_values)\n",
    "  step_size = tf.get_variable(name='step_size', initializer=tf.constant(0.01, dtype=dtype))\n",
    "\n",
    "  # Initialize the HMC sampler, now wrapped in the TransformedTransitionKernel\n",
    "  ttk = tfp.mcmc.TransformedTransitionKernel(\n",
    "      inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "      target_log_prob_fn=log_lik_fn,\n",
    "      step_size=step_size,\n",
    "      num_leapfrog_steps=3),\n",
    "      bijector=[unconstrained_to_precision])\n",
    "\n",
    "  # One iteration\n",
    "  next_x, other_results = ttk.one_step(\n",
    "      current_state=x,\n",
    "      previous_kernel_results=ttk.bootstrap_results(x))\n",
    "\n",
    "  x_update = x.assign(next_x)\n",
    "\n",
    "  # Adapt the step size using standard adaptive MCMC procedure. See Section 4.2\n",
    "  # of Andrieu and Thoms (2008):\n",
    "  # http://www4.ncsu.edu/~rsmith/MA797V_S12/Andrieu08_AdaptiveMCMC_Tutorial.pdf\n",
    "\n",
    "  # NOTE: one change from above is that we have to look at \n",
    "  # other_results.inner_results.log_accept_ratio, since the new kernel\n",
    "  # wraps the results from the HMC kernel.\n",
    "  step_size_update = step_size.assign_add(\n",
    "      step_size * tf.where(\n",
    "          tf.exp(tf.minimum(tf.reduce_mean(\n",
    "              other_results.inner_results.log_accept_ratio), 0.)) >\n",
    "              target_accept_rate,\n",
    "          x=0.1, y=-0.1))\n",
    "\n",
    "  # Note, the adaptations are performed during warmup only.\n",
    "  warmup = tf.group([x_update, step_size_update])\n",
    "\n",
    "  init = tf.global_variables_initializer()\n",
    "  \n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  # Initialize\n",
    "  sess.run(init)\n",
    "\n",
    "  # Warm up the sampler and adapt the step size\n",
    "  print 'Warmup'\n",
    "  start_time = time.time()\n",
    "  for i in range(num_warmup_iter):\n",
    "    sess.run(warmup)\n",
    "    if i % 500 == 0:\n",
    "      print 'Step %d' % i\n",
    "  end_time = time.time()\n",
    "  print 'Time per step:', (end_time - start_time) / num_warmup_iter\n",
    "  print 'Step size: %g' % sess.run(step_size)\n",
    "\n",
    "  # Collect samples without adapting step size\n",
    "  print 'Sampling'\n",
    "  start_time = time.time()\n",
    "  precision_samples = np.zeros([num_chain_iter, N_CHAINS, 2, 2])\n",
    "  results = []\n",
    "  for i in range(num_chain_iter):\n",
    "    _, x_, other_results_ = sess.run([x_update, x, other_results])\n",
    "    precision_samples[i, :] = x_\n",
    "    results.append(other_results_)\n",
    "    if i % 500 == 0:\n",
    "      print 'Step %d' % i\n",
    "  end_time = time.time()\n",
    "  print 'Time per step:', (end_time - start_time) / num_chain_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sufld3q7kPV1"
   },
   "source": [
    "### Checking convergence\n",
    "\n",
    "The $\\hat{R}$ convergence check looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "a3Ou0K51jRfz",
    "outputId": "c21d4924-a545-4f04-e6d6-a79c71e16783"
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "  r_hat = tfp.mcmc.potential_scale_reduction(precision_samples)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  print sess.run(r_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7Vy6BuelDPZ"
   },
   "source": [
    "### Comparison against the analytic posterior\n",
    "\n",
    "Again let's check against the analytic posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXxuO15HkeTD"
   },
   "outputs": [],
   "source": [
    "# The output samples have shape [n_steps, n_chains, 2, 2]\n",
    "# Flatten them to [n_steps * n_chains, 2, 2] via reshape:\n",
    "precision_samples_reshaped = np.reshape(precision_samples, [-1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "twDknslqk1uG",
    "outputId": "51a01dc2-9e22-4d82-c0e5-84a976ee3a57"
   },
   "outputs": [],
   "source": [
    "print 'True posterior mean:\\n', posterior_mean\n",
    "print 'Mean of samples:\\n', np.mean(precision_samples_reshaped, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "22448rkUk_Wa",
    "outputId": "741cfb0c-2f61-4c10-b82f-01ced9cfb56b"
   },
   "outputs": [],
   "source": [
    "print 'True posterior standard deviation:\\n', posterior_sd\n",
    "print 'Standard deviation of samples:\\n', np.std(precision_samples_reshaped, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TwR4z4glG0J"
   },
   "source": [
    "# Optimizations\n",
    "\n",
    "Now that we've got things running end-to-end, let's do a more optimized version.  Speed doesn't matter too much for this example, but once matrices get larger, a few optimizations will make a big difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbZv_23gT3oG"
   },
   "source": [
    "One big speed improvement we can make is to reparameterize in terms of the Cholesky decomposition.  The reason is our data likelihood function requires both the covariance and the precision matrices.  Matrix inversion is expensive ($O(n^3)$ for an $n \\times n$ matrix), and if we parameterize in terms of either the covariance or the precision matrix, we need to do an inversion to get the other.\n",
    "\n",
    "As a reminder, a real, positive-definite, symmetric matrix $M$ can be decomposed into a product of the form $M = L L^T$ where the matrix $L$ is lower triangular and has positive diagonals.  Given the Cholesky decomposition of $M$, we can more efficiently obtain both $M$ (the product of a lower and an upper triangular matrix) and $M^{-1}$ (via back-substitution).  The Cholesky factorization itself is not cheap to compute, but if we parameterize in terms of Cholesky factors, we only need to compute the Choleksy factorization of the initial parameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgYTtadL7sMf"
   },
   "source": [
    "## Using the Cholesky decomposition of the covariance matrix\n",
    "\n",
    "TFP has a version of the multivariate normal distribution, [MultivariateNormalTriL](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalTriL), that is parameterized in terms of the Cholesky factor of the covariance matrix.  So if we were to parameterize in terms of the Cholesky factor of the covariance matrix, we could compute the data log likelihood efficiently.  The challenge is in computing the prior log likelihood with similar efficiency.\n",
    "\n",
    "If we had a version of the inverse Wishart distribution that worked with Cholesky factors of samples, we'd be all set.  Alas, we don't.  (The team would welcome code submissions, though!)  As an alternative, we can use a version of the Wishart distribution that works with Cholesky factors of samples together with a chain of bijectors.\n",
    "\n",
    "At the moment, we're missing a few stock bijectors to make things really efficient, but I want to show the process as an exercise and a useful illustration of the power of TFP's bijectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0OWSj5HCEA6"
   },
   "source": [
    "### A Wishart distribution that operates on Cholesky factors\n",
    "\n",
    "The `Wishart` distribution has a useful flag, `input_output_cholesky`, that specifies that the input and output matrices should be Cholesky factors.  It's more efficient and numerically advantageous to work with the Cholesky factors than full matrices, which is why this is desirable.  An important point about the semantics of the flag: it's only an indication that the representation of the input and output to the distribution should change - it does *not* indicate a full reparameterization of the distribution, which would involve a Jacobian correction to the `log_prob()` function.  We actually want to do this full reparameterization, so we'll build our own distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgUyMB4OEyFZ"
   },
   "outputs": [],
   "source": [
    "# An optimized Wishart distribution that has been transformed to operate on\n",
    "# Cholesky factors instead of full matrices.  Note that we gain a modest\n",
    "# additional speedup by specifying the Cholesky factor of the scale matrix\n",
    "# (i.e. by passing in the scale_tril parameter instead of scale).\n",
    "\n",
    "class CholeskyWishart(tfd.TransformedDistribution):\n",
    "  \"\"\"Wishart distribution reparameterized to use Cholesky factors.\"\"\"\n",
    "  def __init__(self,\n",
    "      df,\n",
    "      scale_tril,\n",
    "      validate_args=False,\n",
    "      allow_nan_stats=True,\n",
    "      name='CholeskyWishart'):\n",
    "    # Wishart has a bunch of methods that we want to support but not\n",
    "    # implement.  We'll subclass TransformedDistribution here to take care of\n",
    "    # those.  We'll override the few for which speed is critical and implement\n",
    "    # them with a separate Wishart for which input_output_cholesky=True\n",
    "    super(CholeskyWishart, self).__init__(\n",
    "        distribution=tfd.Wishart(\n",
    "            df=df,\n",
    "            scale_tril=scale_tril,\n",
    "            input_output_cholesky=False,\n",
    "            validate_args=validate_args,\n",
    "            allow_nan_stats=allow_nan_stats),\n",
    "        bijector=tfb.Invert(tfb.CholeskyOuterProduct()),\n",
    "        validate_args=validate_args,\n",
    "        name=name\n",
    "    )\n",
    "    # Here's the Cholesky distribution we'll use for log_prob() and sample()\n",
    "    self.cholesky = tfd.Wishart(\n",
    "        df=df,\n",
    "        scale_tril=scale_tril,\n",
    "        input_output_cholesky=True,\n",
    "        validate_args=validate_args,\n",
    "        allow_nan_stats=allow_nan_stats)\n",
    "    \n",
    "  def _log_prob(self, x):\n",
    "    return (self.cholesky.log_prob(x) +\n",
    "            self.bijector.inverse_log_det_jacobian(x, event_ndims=2))\n",
    "\n",
    "  def _sample_n(self, n, seed=None):\n",
    "    return self.cholesky._sample_n(n, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "yVBT4gXwNZbS",
    "outputId": "99ff16a8-cd1f-420d-ad98-1db50c52e135"
   },
   "outputs": [],
   "source": [
    "# some checks\n",
    "PRIOR_SCALE_CHOLESKY = np.linalg.cholesky(PRIOR_SCALE)\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "  w_transformed = tfd.TransformedDistribution(\n",
    "      tfd.Wishart(df=PRIOR_DF, scale_tril=PRIOR_SCALE_CHOLESKY),\n",
    "      bijector=tfb.Invert(tfb.CholeskyOuterProduct()))\n",
    "  w_optimized = CholeskyWishart(\n",
    "      df=PRIOR_DF, scale_tril=PRIOR_SCALE_CHOLESKY)\n",
    "\n",
    "  m = tf.placeholder(dtype=tf.float32)\n",
    "  log_prob_transformed = w_transformed.log_prob(m)\n",
    "  log_prob_optimized = w_optimized.log_prob(m)\n",
    "  \n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  for matrix in [np.eye(2, dtype=np.float32),\n",
    "                 np.array([[1., 0.], [2., 8.]], dtype=np.float32)]:\n",
    "    log_prob_transformed_, log_prob_optimized_ = sess.run(\n",
    "        [log_prob_transformed, log_prob_optimized],\n",
    "        feed_dict={m: matrix})\n",
    "    print 'Transformed Wishart:', log_prob_transformed_\n",
    "    print 'Optimized Wishart', log_prob_optimized_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpKd8zkW75sm"
   },
   "source": [
    "### Building an inverse Wishart distribution\n",
    "\n",
    "We have our covariance matrix $C$ decomposed into $C = L L^T$ where $L$ is lower triangular and has a positive diagonal.  We want to know the probability of $L$ given that $C \\sim W^{-1}(\\nu, V)$ where $W^{-1}$ is the inverse Wishart distribution.\n",
    "\n",
    "The inverse Wishart distribution has the property that if $C \\sim W^{-1}(\\nu, V)$, then the precision matrix $C^{-1} \\sim W(\\nu, V^{-1})$.  So we can get the probability of $L$ via a `TransformedDistribution` that takes as parameters the Wishart distribution and a bijector that maps the Cholesky factor of precision matrix to a Cholesky factor of its inverse.\n",
    "\n",
    "A straightforward (but not super efficient) way to get from the Cholesky factor of $C^{-1}$ to $L$ is to invert the Cholesky factor by back-solving, then forming the covariance matrix from these inverted factors, and then doing a Cholesky factorization.\n",
    "\n",
    "Let the Cholesky decomposition of $C^{-1} = M M^T$.  $M$ is lower triangular, so we can invert it using the `MatrixInverseTriL` bijector.\n",
    "\n",
    "Forming $C$ from $M^{-1}$ is a little tricky: $C = (M M^T)^{-1} = M^{-T}M^{-1} = M^{-T} (M^{-T})^T$.  $M$ is lower triangular, so $M^{-1}$ will also be lower triangular, and $M^{-T}$ will be upper triangular.  The `CholeskyOuterProduct()` bijector only works with lower triangular matrices, so we can't use it to form $C$ from $M^{-T}$.  Our workaround is a chain of bijectors that permute the rows and columns of a matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CgUuL8kwVFDK"
   },
   "outputs": [],
   "source": [
    "# Here's our permuting bijector:\n",
    "def get_permuter():\n",
    "  permutation = [1, 0]\n",
    "  return tfb.Chain([\n",
    "      tfb.Transpose(rightmost_transposed_ndims=2),\n",
    "      tfb.Permute(permutation=permutation),\n",
    "      tfb.Transpose(rightmost_transposed_ndims=2),\n",
    "      tfb.Permute(permutation=permutation),   \n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "1yjo-oudqAAf",
    "outputId": "1b53d24a-8e4c-4f36-c167-20460798ad80"
   },
   "outputs": [],
   "source": [
    "# Some sanity checks\n",
    "with tf.Graph().as_default() as g:\n",
    "  m = np.array([[1., 0.], [2., 8.]], dtype=np.float32)\n",
    "  permuter = get_permuter()\n",
    "  p_fwd = permuter.forward(m)\n",
    "  p_fwd_fwd = permuter.forward(p_fwd)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  print 'm =\\n', m\n",
    "  print 'permuted = \\n', sess.run(p_fwd)\n",
    "  print 'permuted^2 = \\n', sess.run(p_fwd_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "furWzJRYvFYC"
   },
   "source": [
    "### Combining all the pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6-vpGOBTqhr"
   },
   "source": [
    "Our final bijector is now a big chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPPO17cgq7SK"
   },
   "outputs": [],
   "source": [
    "def get_wishart_cholesky_to_iw_cholesky():\n",
    "  return tfb.Chain([\n",
    "      # step 6: get the Cholesky factor for the covariance matrix\n",
    "      tfb.Invert(tfb.CholeskyOuterProduct()),\n",
    "      # step 5: undo our permutation (note that permuter.inverse = permuter.forward)\n",
    "      get_permuter(),\n",
    "      # step 4: form the covariance matrix from the inverted Cholesky factors\n",
    "      tfb.CholeskyOuterProduct(),\n",
    "      # step 3: make things lower triangular\n",
    "      get_permuter(),\n",
    "      # step 2: transpose the inverse\n",
    "      tfb.Transpose(rightmost_transposed_ndims=2),\n",
    "      # step 1: invert the Cholesky factor (see code below)\n",
    "      tfb.MatrixInverseTriL()\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "QDCeOcQOrIUI",
    "outputId": "2758975a-d3ea-46dc-dd48-c6bccaa415c2"
   },
   "outputs": [],
   "source": [
    "# verify that the bijector works\n",
    "with tf.Graph().as_default() as g:\n",
    "  m = np.array([[1., 0.], [2., 8.]], dtype=np.float32)\n",
    "  c_inv = m.dot(m.T)\n",
    "  c = np.linalg.inv(c_inv)\n",
    "  c_chol = np.linalg.cholesky(c)\n",
    "  wishart_cholesky_to_iw_cholesky = get_wishart_cholesky_to_iw_cholesky()\n",
    "  w_fwd = wishart_cholesky_to_iw_cholesky.forward(m)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  print 'numpy =\\n', c_chol\n",
    "  print 'bijector =\\n', sess.run(w_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CF7IFULOvHy3"
   },
   "source": [
    "### Our final distribution\n",
    "\n",
    "Our inverse Wishart operating on Cholesky factors is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJXoPZ1e-8yh"
   },
   "outputs": [],
   "source": [
    "inverse_wishart_cholesky = tfd.TransformedDistribution(\n",
    "    distribution=CholeskyWishart(\n",
    "        df=PRIOR_DF,\n",
    "        scale_tril=np.linalg.cholesky(np.linalg.inv(PRIOR_SCALE))),\n",
    "    bijector=get_wishart_cholesky_to_iw_cholesky())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUC-6bPEapDg"
   },
   "source": [
    "We've got our inverse Wishart, but it's kind of slow because we have to do a Cholesky decomposition in the bijector.  Let's return to the precision matrix parameterization and see what we can do there for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIyb_aYyCzZa"
   },
   "source": [
    "# Final(!) Version: using the Cholesky decomposition of the precision matrix\n",
    "\n",
    "An alternative approach is to work with Cholesky factors of the precision matrix.  Here the prior likelihood function is easy to compute, but the data log likelihood function takes more work since TFP doesn't have a version of the multivariate normal that is parameterized by precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSOOuAgZtSu5"
   },
   "source": [
    "### Optimized prior log likelihood\n",
    "\n",
    "We use the `CholeskyWishart` distribution we built above to construct the prior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8V5hA9SUqHy"
   },
   "outputs": [],
   "source": [
    "# Our new prior.\n",
    "PRIOR_SCALE_CHOLESKY = np.linalg.cholesky(PRIOR_SCALE)\n",
    "\n",
    "def log_lik_prior_cholesky(precisions_cholesky):\n",
    "  rv_precision = CholeskyWishart(\n",
    "      df=PRIOR_DF,\n",
    "      scale_tril=PRIOR_SCALE_CHOLESKY,\n",
    "      validate_args=VALIDATE_ARGS,\n",
    "      allow_nan_stats=ALLOW_NAN_STATS)\n",
    "  return rv_precision.log_prob(precisions_cholesky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "jn2ey0JdjzhW",
    "outputId": "fa1db665-8996-42a0-fb59-1fdb2f07248d"
   },
   "outputs": [],
   "source": [
    "# Check against the slower TF implementation and the NumPy implementation.\n",
    "# Note that when comparing to NumPy, we need to add in the Jacobian correction.\n",
    "with tf.Graph().as_default() as g:\n",
    "  precisions = [np.eye(2, dtype=np.float32),\n",
    "                true_precision]\n",
    "  precisions_cholesky = np.stack([np.linalg.cholesky(m) for m in precisions])\n",
    "  precisions = np.stack(precisions)\n",
    "  lik_tf = log_lik_prior_cholesky(precisions_cholesky)\n",
    "  lik_tf_slow = tfd.TransformedDistribution(\n",
    "      distribution=tfd.Wishart(df=PRIOR_DF, scale=PRIOR_SCALE),\n",
    "      bijector=tfb.Invert(tfb.CholeskyOuterProduct())).log_prob(\n",
    "      precisions_cholesky)\n",
    "  corrections = tfb.Invert(tfb.CholeskyOuterProduct()).inverse_log_det_jacobian(\n",
    "      precisions_cholesky, event_ndims=2)\n",
    "  n = precisions.shape[0]\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  lik_tf_, lik_tf_slow_, corrections_ = sess.run(\n",
    "      [lik_tf, lik_tf_slow, corrections])\n",
    "  for i in range(n):\n",
    "    print i\n",
    "    print 'numpy:', log_lik_prior_numpy(precisions[i]) + corrections_[i]\n",
    "    print 'tensorflow slow:', lik_tf_slow_[i]      \n",
    "    print 'tensorflow fast:', lik_tf_[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjjqZccAPaVG"
   },
   "source": [
    "### Optimized data log likelihood\n",
    "\n",
    "We can use TFP's bijectors to build our own version of the multivariate normal.  Here is the key idea:\n",
    "\n",
    "Suppose I have a column vector $X$ whose elements are iid samples of $N(0, 1)$.  We have $\\text{mean}(X) = 0$ and $\\text{cov}(X) = I$\n",
    "\n",
    "Now let $Y = A X + b$.  We have $\\text{mean}(Y) = b$ and $\\text{cov}(Y) = A A^T$\n",
    "\n",
    "Hence we can make vectors with mean $b$ and covariance $C$ using the affine transform $Ax+b$ to vectors of iid standard Normal samples provided $A A^T = C$.  The Cholesky decomposition of $C$ has the desired property.  However, there are other solutions.\n",
    "\n",
    "Let $P = C^{-1}$ and let the Cholesky decomposition of $P$ be $B$, i.e. $B B^T = P$.  Now\n",
    "\n",
    "$P^{-1} = (B B^T)^{-1} = B^{-T} B^{-1} = B^{-T} (B^{-T})^T$\n",
    "\n",
    "So another way to get our desired mean and covariance is to use the affine transform $Y=B^{-T}X + b$.\n",
    "\n",
    "Our approach (courtesy of [this notebook](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb)):\n",
    "1. Use `tfd.Independent()` to combine a batch of 1-D `Normal` random variables into a single multi-dimensional random variable. The `reinterpreted_batch_ndims` parameter for `Independent()` specifies the number of batch dimensions that should be reinterpreted as event dimensions.  In our case we create a 1-D batch of length 2 that we transform into a 1-D event of length 2, so `reinterpreted_batch_ndims=1`.\n",
    "2. Apply a bijector to add the desired covariance: `tfb.Invert(tfb.Affine(scale_tril=precision_cholesky, adjoint=True))`.  Note that above we're multiplying our iid normal random variables by the transpose of the inverse of the Cholesky factor of the precision matrix $(B^{-T}X)$.  The `tfb.Invert` takes care of inverting $B$, and the `adjoint=True` flag performs the transpose.\n",
    "3. Apply a bijector to add the desired offset: `tfb.Affine(shift=shift)`  Note that we have to do the shift as a separate step from the initial inverted affine transform because otherwise the inverted scale is applied to the shift (since the inverse of $y=Ax+b$ is $x=A^{-1}y - A^{-1}b$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLUqa6lvPCIM"
   },
   "outputs": [],
   "source": [
    "class MVNPrecisionCholesky(tfd.TransformedDistribution):\n",
    "  \"\"\"Multivariate normal parametrized by loc and Cholesky precision matrix.\"\"\"\n",
    "\n",
    "  def __init__(self, loc, precision_cholesky, name=None):\n",
    "    super(MVNPrecisionCholesky, self).__init__(\n",
    "        distribution=tfd.Independent(\n",
    "            tfd.Normal(loc=tf.zeros_like(loc),\n",
    "                       scale=tf.ones_like(loc)),\n",
    "            reinterpreted_batch_ndims=1),\n",
    "        bijector=tfb.Chain([\n",
    "            tfb.Affine(shift=loc),\n",
    "            tfb.Invert(tfb.Affine(scale_tril=precision_cholesky,\n",
    "                                  adjoint=True)),\n",
    "        ]),\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rp-71gFUdUz"
   },
   "outputs": [],
   "source": [
    "def log_lik_data_cholesky(precisions_cholesky, replicated_data):\n",
    "  n = tf.shape(precisions_cholesky)[0]  # number of precision matrices\n",
    "  rv_data = MVNPrecisionCholesky(\n",
    "      loc=tf.zeros([n, 2]),\n",
    "      precision_cholesky=precisions_cholesky)\n",
    "  return tf.reduce_sum(rv_data.log_prob(replicated_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "wp9qyJebgHXu",
    "outputId": "bea24a3c-6aaf-451c-8650-8b7ef01e874e"
   },
   "outputs": [],
   "source": [
    "# check against the numpy implementation\n",
    "with tf.Graph().as_default() as g:\n",
    "  true_precision_cholesky = np.linalg.cholesky(true_precision)\n",
    "  precisions = [np.eye(2, dtype=np.float32), true_precision]\n",
    "  precisions_cholesky = np.stack([np.linalg.cholesky(m) for m in precisions])\n",
    "  precisions = np.stack(precisions)\n",
    "  n = precisions_cholesky.shape[0]\n",
    "  replicated_data = np.tile(np.expand_dims(my_data, axis=1), reps=[1, 2, 1])\n",
    "  lik_tf = log_lik_data_cholesky(precisions_cholesky, replicated_data)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  lik_tf_ = sess.run(lik_tf)\n",
    "  for i in range(n):\n",
    "    print i\n",
    "    print 'numpy:', log_lik_data_numpy(precisions[i], my_data)\n",
    "    print 'tensorflow:', lik_tf_[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHZY4zxOt5BH"
   },
   "source": [
    "### Combined log likelihood function\n",
    "\n",
    "Now we combine our prior and data log likelihood functions in a closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqx8TS2wYTYh"
   },
   "outputs": [],
   "source": [
    "def get_log_lik_cholesky(data, n_chains=1):\n",
    "  # The data argument that is passed in will be available to the inner function\n",
    "  # below so it doesn't have to be passed in as a parameter.\n",
    "  replicated_data = np.tile(np.expand_dims(data, axis=1), reps=[1, n_chains, 1])\n",
    "\n",
    "  def _log_lik_cholesky(precisions_cholesky):\n",
    "    return (log_lik_data_cholesky(precisions_cholesky, replicated_data) +\n",
    "            log_lik_prior_cholesky(precisions_cholesky))\n",
    "\n",
    "  return _log_lik_cholesky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cig7gNSUt7ls"
   },
   "source": [
    "### Constraining bijector\n",
    "\n",
    "Our samples are constrained to be valid Cholesky factors, which means they must be lower triangular matrices with positive diagonals.  The `TransformedTransitionKernel` needs a bijector that maps unconstrained tensors to/from tensors with our desired constraints.  We've removed the Cholesky decomposition from the bijector's inverse, which speeds things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Nva4oOGTjN_"
   },
   "outputs": [],
   "source": [
    "unconstrained_to_precision_cholesky = tfb.Chain([\n",
    "    # step 2: exponentiate the diagonals    \n",
    "    tfb.TransformDiagonal(tfb.Exp(validate_args=VALIDATE_ARGS)),\n",
    "    # step 1: expand the vector to a lower triangular matrix\n",
    "    tfb.FillTriangular(validate_args=VALIDATE_ARGS),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "oXcs29Znlzvx",
    "outputId": "b7244ad0-c661-4c01-e753-86027ea6717e"
   },
   "outputs": [],
   "source": [
    "# some checks\n",
    "with tf.Graph().as_default() as g:\n",
    "  inv = unconstrained_to_precision_cholesky.inverse(precisions_cholesky)\n",
    "  fwd = unconstrained_to_precision_cholesky.forward(inv)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  inv_, fwd_ = sess.run([inv, fwd])\n",
    "  print 'precisions_cholesky:\\n', precisions_cholesky\n",
    "  print '\\ninv:\\n', inv_\n",
    "  print '\\nfwd(inv):\\n', fwd_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HU6SK7DMuR6B"
   },
   "source": [
    "### Initial values\n",
    "\n",
    "We generate a tensor of initial values.  We're working with Cholesky factors, so we generate some Cholesky factor initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIOjT1HxZg0C"
   },
   "outputs": [],
   "source": [
    "# The number of chains is determined by the shape of the initial values.\n",
    "# Here we'll generate 3 chains, so we'll need a tensor of 3 initial values.\n",
    "N_CHAINS = 3\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "initial_values_cholesky = []\n",
    "for i in range(N_CHAINS):\n",
    "  initial_values_cholesky.append(np.array(\n",
    "      [[0.5 + np.random.uniform(), 0.0],\n",
    "       [-0.5 + np.random.uniform(), 0.5 + np.random.uniform()]],\n",
    "      dtype=np.float32))\n",
    "initial_values_cholesky = np.stack(initial_values_cholesky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VakfU-Tyuic4"
   },
   "source": [
    "### Sampling\n",
    "\n",
    "We sample N_CHAINS chains using the `TransformedTransitionKernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "aFzFjNIoYre3",
    "outputId": "40807f56-267b-4aca-bebe-b434561a47b6"
   },
   "outputs": [],
   "source": [
    "# Code adapted from tensorflow_probability/python/mcmc/hmc.py\n",
    "with tf.Graph().as_default() as g:\n",
    "  tf.set_random_seed(123)\n",
    "  log_lik_fn = get_log_lik_cholesky(my_data)\n",
    "\n",
    "  # Tuning acceptance rates:\n",
    "  dtype = np.float32\n",
    "  num_warmup_iter = 2500\n",
    "  num_chain_iter = 2500\n",
    "\n",
    "  # Set the target average acceptance ratio for the HMC as suggested by\n",
    "  # Beskos et al. (2013):\n",
    "  # https://projecteuclid.org/download/pdfview_1/euclid.bj/1383661192\n",
    "  target_accept_rate = 0.651\n",
    "\n",
    "  x = tf.get_variable(name='x', initializer=initial_values_cholesky)\n",
    "  step_size = tf.get_variable(name='step_size',\n",
    "                              initializer=tf.constant(0.01, dtype=dtype))\n",
    "\n",
    "  # Initialize the HMC sampler, now wrapped in the TransformedTransitionKernel\n",
    "  ttk = tfp.mcmc.TransformedTransitionKernel(\n",
    "      inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "      target_log_prob_fn=log_lik_fn,\n",
    "      step_size=step_size,\n",
    "      num_leapfrog_steps=3),\n",
    "      bijector=[unconstrained_to_precision_cholesky])\n",
    "\n",
    "  # One iteration\n",
    "  next_x, other_results = ttk.one_step(\n",
    "      current_state=x,\n",
    "      previous_kernel_results=ttk.bootstrap_results(x))\n",
    "\n",
    "  x_update = x.assign(next_x)\n",
    "  precision = tf.matmul(x, x, transpose_b=True)\n",
    "\n",
    "  # Adapt the step size using standard adaptive MCMC procedure. See Section 4.2\n",
    "  # of Andrieu and Thoms (2008):\n",
    "  # http://www4.ncsu.edu/~rsmith/MA797V_S12/Andrieu08_AdaptiveMCMC_Tutorial.pdf\n",
    "\n",
    "  # NOTE: one change from above is that we have to look at \n",
    "  # other_results.inner_results.log_accept_ratio, since the new kernel\n",
    "  # wraps the results from the HMC kernel.\n",
    "  step_size_update = step_size.assign_add(\n",
    "      step_size * tf.where(\n",
    "          tf.exp(tf.minimum(tf.reduce_mean(\n",
    "              other_results.inner_results.log_accept_ratio), 0.)) >\n",
    "              target_accept_rate,\n",
    "          x=0.1, y=-0.1))\n",
    "\n",
    "  # Note, the adaptations are performed during warmup only.\n",
    "  warmup = tf.group([x_update, step_size_update])\n",
    "\n",
    "  init = tf.global_variables_initializer()\n",
    "  \n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  # Initialize\n",
    "  sess.run(init)\n",
    "\n",
    "  # Warm up the sampler and adapt the step size\n",
    "  print 'Warmup'\n",
    "  start_time = time.time()\n",
    "  for i in range(num_warmup_iter):\n",
    "    sess.run(warmup)\n",
    "    if i % 500 == 0:\n",
    "      print 'Step %d' % i\n",
    "  end_time = time.time()\n",
    "  print 'Time per step:', (end_time - start_time) / num_warmup_iter\n",
    "  print 'Step size: %g' % sess.run(step_size)\n",
    "\n",
    "  # Collect samples without adapting step size\n",
    "  print 'Sampling'\n",
    "  start_time = time.time()\n",
    "  precision_samples = np.zeros([num_chain_iter, N_CHAINS, 2, 2])\n",
    "  results = []\n",
    "  for i in range(num_chain_iter):\n",
    "    _, precision_, other_results_ = sess.run(\n",
    "        [x_update, precision, other_results])\n",
    "    precision_samples[i, :] = precision_\n",
    "    results.append(other_results_)\n",
    "    if i % 500 == 0:\n",
    "      print 'Step %d' % i\n",
    "  end_time = time.time()\n",
    "  print 'Time per step:', (end_time - start_time) / num_chain_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9V5D9rCgurdK"
   },
   "source": [
    "### Convergence check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKcXls81qwig"
   },
   "source": [
    "A quick convergence check looks good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8S70eeurquGj",
    "outputId": "b6ff20bd-1941-470e-e84e-31e8b38b4e75"
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "  r_hat = tfp.mcmc.potential_scale_reduction(precision_samples)\n",
    "  g.finalize()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  print 'r_hat:\\n', sess.run(r_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jX-PqDluuaZ"
   },
   "source": [
    "### Comparing results to the analytic posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "merfcOkkrKMS"
   },
   "outputs": [],
   "source": [
    "# The output samples have shape [n_steps, n_chains, 2, 2]\n",
    "# Flatten them to [n_steps * n_chains, 2, 2] via reshape:\n",
    "precision_samples_reshaped = np.reshape(precision_samples, newshape=[-1, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f15131B3skLj"
   },
   "source": [
    "And again, the sample means and standard deviations match those of the analytic posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ZzDUnYjLsDI3",
    "outputId": "434da413-e122-43ae-9401-d290d4dfb197"
   },
   "outputs": [],
   "source": [
    "print 'True posterior mean:\\n', posterior_mean\n",
    "print 'Mean of samples:\\n', np.mean(precision_samples_reshaped, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "C5ztGXbrsNSe",
    "outputId": "adddb44b-9d89-42e3-9d99-6485f6302741"
   },
   "outputs": [],
   "source": [
    "print 'True posterior standard deviation:\\n', posterior_sd\n",
    "print 'Standard deviation of samples:\\n', np.std(precision_samples_reshaped, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqFQDMOHsXED"
   },
   "source": [
    "Ok, all done!  We've got our optimized sampler working."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlow_Probability_Case_Study_Covariance_Estimation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
